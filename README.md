# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)

<!-- #region -->
## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset provides data about bank marketing info. We build a model that can predict whether a customer will agree to open a term deposit account with the bank or not.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was created by AutoML.
We are using accuracy as evaluation metric, so the highest accuracy of about 0.917602 was achieved by AutoML, which outperformed the other methods.


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

Using Tabular to import data from specified URL.
The logistic regression model used random selection for its two parameters. This means that the values of the parameters were not predetermined, but randomly.
the hyperparameter values randomly from the search space we defined. We set the inverse of regularization strength where 1 is the default value. We also set the maximum number of iterations where 100 is the default value.

**What are the benefits of the parameter sampler you chose?**

Random sampling is straightforward to implement. No complex algorithms or heuristics are needed.
It’s a good starting point for hyperparameter tuning, especially when little prior knowledge about the problem exists.

**What are the benefits of the early stopping policy you chose?**

The policy applies at a specified frequency called the evaluation interval.
Any run that doesn’t fall within the slack factor or slack amount of the evaluation metric (compared to the best performing run) will be terminated.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

One of the main advantages of AutoML is its ability to automatically search through a large space of models and hyperparameters to find the best-performing model for a given dataset.


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Comparing the performance of the scikit-learn pipeline and the autoML pipeline on the same data set. The scikit-learn pipeline uses a logistic regression model with various hyperparameters, while the autoML pipeline tries different models and selects the best one. Both pipelines apply the same data cleaning steps. The accuracy scores are very close, with the autoML pipeline (accuracy: 0.9176) slightly outperforming the scikit-learn pipeline (accuracy: 0.9156) by 0.020 points.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

For future experiments, I will try different primary metrics, classification methods, calculate and compare the mean squared error, which measures how much our predictions deviate from the actual values. 
I will try to use different models and model hyperparameters, different training or evaluation data, each of these experiments can produce completely different evaluation metrics. 

About the infrastructure, I will explore the effect of increasing the number of clusters used to speed up the analysis. These steps could help to reduce the error in our model and study it more efficiently.

<!-- #endregion -->
